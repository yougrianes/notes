论文*Learning from Active Human Involvement through Proxy Value Propagation*提出了一种名为代理价值传播（Proxy Value Propagation, PVP）的无奖励主动人类参与学习方法，能将基于价值的强化学习方法转化为高效的无奖励的人在回路策略优化方法，通过实验证明该方法在多种任务和控制设备上具有优异性能和较高学习效率。
1. **研究背景**：强化学习（RL）在诸多领域有应用，但手动设计的奖励函数难以完全表达人类意图，可能导致智能体行为偏差。人在回路学习（HL）可克服奖励工程的问题，其中主动人类参与能增强系统的安全性和AI对齐性。
2. **相关工作**：偏好学习通过轨迹排序学习人类偏好，但未充分利用实时反馈；被动人类参与的HL方法存在安全风险；主动人类参与的方法虽有研究，但此前方法在学习效率、泛化性等方面存在不足。
3. **问题公式化**：政策学习旨在解决顺序决策问题，通常用马尔可夫决策过程（MDP）建模。传统RL通过最大化期望累积回报学习策略，但奖励函数存在缺陷。模仿学习（IL）从人类行为中学习策略，人在回路的主动学习可引入干预策略，缓解分布转移问题。目标是找到与人类偏好一致的新手策略，同时最小化人类干预。
4. **方法**
    - **代理价值传播（PVP）**：将基于价值的RL方法转化为无奖励的人在回路策略优化方法。训练时，人类监督智能体与环境的交互，记录相关数据。通过给人类动作和被干预的智能体动作分别标记高、低Q值，利用代理价值损失（PV loss）拟合Q网络，并结合TD学习传播代理价值，最终通过总价值损失更新策略。
    - **分析**：PVP可看作在无奖励在线学习设置下采用保守Q学习（CQL）目标，并增加L2正则项。与直接分配奖励的方法相比，PVP更具可行性。
    - **实现细节**：基于TD3和DQN扩展实现PVP，采用确定性新手政策，平衡人类和智能体数据缓冲区，避免关键人类行为学习效率低和灾难性遗忘问题。
5. **实验**
    - **实验设置**：在多个具有不同观察和动作空间的控制任务上实验，使用多种控制设备，设置多种评估指标，对比多种基线方法。
    - **基线比较**：与RL方法相比，PVP在各任务中性能更优、学习效率更高；与其他人在回路基线方法相比，PVP在测试性能上表现出色，且人力成本更低。
    - **用户研究**：PVP在合规性、性能和减轻人类训练压力方面表现最佳，用户友好度最高。
    - **消融研究**：验证了TD学习、平衡缓冲区、新手缓冲区、确定性政策以及Q值正则化对PVP性能的重要性。
6. **研究结论**：PVP能有效从主动人类参与的干预和反馈中学习，可无缝集成到现有基于价值的RL方法中，实现高效的无奖励政策学习。但该方法存在局限性，如仅应用于两种基于价值的RL方法、不适用于人类无法演示的任务等 。 