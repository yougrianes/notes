# sunhao talk notes

近十年间，强化学习（RL）在棋类、即时策略游戏及系统性能优化等任务中表现出色，总能找到优于人类专家的策略，具备极致优化能力与持续提升潜力，其成果还能为人类专家带来启发，如 AlphaGo 的 “Move 37” 体现了 RL 的创造力。

与此同时，数据驱动的生成模型借助更好的架构、优化器、算力和算法等不断发展，像 Sora、StableDiffusion、GPT 等模型已能按用户指令生成满意内容。

不过，世界数据总量有限，数据驱动的生成模型虽有泛化、指令跟随和可解释等优势，但缺乏 RL 系统的创造力、持续进步和纠错能力，也难以超越人类专家水平。

0.2 RL + LLM?

文章探讨是否存在一种系统，既能像数据驱动的大模型那样理解、帮助人类，又具备不断迭代更新、纠错和变强的能力。

从 LLM4RL 和 RL4LLM 两个角度分析：

    前者可用 LLM 实现超人类表现，以自然语言传递 RL 系统创造力；
    后者能赋予 LLM 性能提升能力，且 post-train/alignment 的优化方向与 RL 学习范式契合。

在数学、通用聊天等领域，RL + LLM 已取得成功：

    如 AlphaProof+Alpha Geometry2 获 IMO 银牌；
    RLHF 研究及 OpenAI 利用用户数据改进体验。

文章进而提出将 RL + LLM 范式推广面临的困难与潜在解决方案是此前教程重点。当前 LLM Alignment 是数据（人类经验）驱动的 RL，Inverse RL 是自然简单方案。同时强调对 IRLxLLM 的研究不能局限于简单自然的方案，还需探索未来优化方向。

相比LLM，人类的学习看似更为“轻松”。人类无需也无法阅览所有书籍、观看所有电视电影，或是踏遍世界的每一个角落，却依然能够拥有高度智能，实现对世界的理解、推理、创造、交流与学习。

人类的学习过程如下：

- 在成长初期，通过语言学习、交互来理解外界；
- 同时，借助与世界的简单互动，掌握基础的“物理”知识（世界模型、规律）；
- 随着成长，学会书写和文字运用，在游戏与虚拟世界中持续学习；
- 最终，通过与现实世界及社会的广泛交互，不断提升自身能力。

这一过程恰好对应LLM+RL发展的四个阶段：**Data-Driven（数据驱动）、Game（游戏）、Virtual Interaction（虚拟交互）、Physical Interaction（物理交互）**。不过需要注意的是，除幼儿时期语言学习明显早于其他学习方式外，人类后续的学习往往是持续且同步进行的，这种层级递进关系并非绝对成立。从LLM迈向AGI的角度划分这几个层次，主要是基于实现难度和安全可控性的考量。

当下，主流方法仍处于AGI的第一层，即通过Data-Driven Reward Model（数据驱动奖励模型）与RL（强化学习）结合，提升任务性能。接下来，我们将从这一层级展开探讨。

第一层：【Data-Driven RL】(Human-Centered) RL with Data-Driven Reward Model

### RL基础与解决思路

从统计视角，RL研究在动态数据分布中主动学习建模（涵盖策略与环境建模，前者支持黑盒策略推理，后者用于规划）；从RL专业表述，即探索如何在与环境交互中获取长期回报最高的策略。

解决RL问题时，各类方法致力于在探索（环境或策略）与利用间寻求平衡。由于针对任一探索策略都能设计反例证明其非最优，随机性便成应对反制设计的有力手段。以MaxEntropy类方法为例，通过与**随机策略保持KL散度维持探索**，使其在多种环境中表现稳健。

RL旨在优化“长期回报”，但多数任务缺乏明确的回报定义，因此难以直接从环境交互中优化策略，常借助LLM从人类语言数据中学习。主要方法有两类：

1. **模仿学习**：如Behavior Clone，通过监督学习模仿行为数据生成相同行为模式。
2. **逆强化学习**：先依据行为数据推断行为试图优化的奖励函数，再基于该函数用RL生成相同行为模式。

### Post-Train的主要内容

1. **Behavior Clone**：Pre-train模型以预测下一个token为任务，本质是模仿人类语料库的Behavior Clone。随着训练规模扩大，模型理解语义能力增强，掌握更有效的embedding模式，在新任务中具备少样本甚至零样本学习能力。
2. **Prompt Engineering**：Post-train初期聚焦于prompt-optimization（上下文学习）。自回归LLM作为条件生成器，输出token的条件概率和分布随输入改变，通过控制输入样本和提问方式，可提升模型在特定任务的表现。但随着模型能力提升，prompt优化边际效应显著，易过拟合测试集，且“lazy prompting”效果提升，需在工程上权衡成本与性能 。
3. **Supervised Fine-Tuning**：若拥有高质量垂类或专家数据，在小规模数据集上进行监督微调（Supervised Fine Tuning）效果良好。该过程简单稳定，适用于资源有限、数据质量高、任务难度适中且对性能改进要求不极致的场景。

Post-train的核心目标是利用少量高质量样本，调整基座模型的回答数据分布，使其适配新任务或特定类型任务。BC和SFT属于直接模仿学习，Prompt-Engineering类似对“成功先验修改经验的模仿”。虽SFT和Prompt-Engineering简单有效，但仍需RL和Reward Model，后续将通过实例阐释其必要性。

    少样本学习（FSL）和零样本学习（ZSL）是机器学习中解决数据稀缺问题的技术：

        少样本学习：用少量训练样本学习和泛化。通过迁移学习（迁移大规模数据上学到的知识）、元学习（让模型学会学习，从多个少样本任务中掌握快速适应新任务的能力）实现。应用在医疗影像分析等数据获取难的领域。
        零样本学习：训练时不接触目标类别样本，借助预训练语言模型或知识图谱等外部知识，依据文本描述、属性信息等建立数据与语义概念的联系，进而对新的、未见过的类别进行分类或预测。用于新兴领域，如新型病毒检测等场景。

#### 用Inverse RL解决Data-Driven RL的原因

在Inverse RL中，关键步骤是依据数据对Reward Model建模，将不完整的MDP（马尔可夫决策过程）\R问题转化为完整的MDP，进而运用RL（强化学习）工具求解。从人类行为数据出发建模奖励函数的过程称为(Neural) Reward Modeling，这是当前主流做法，也是Silver和Sutton文章中提到的以人类为中心的AI。以下通过三个例子说明Reward Model的作用与优势：

1. **收集规模化数据**：以ChatGPT为例，使用GPT时会让用户提供偏好，助力OpenAI优化未来模型。Preference判别任务比生成任务更易拓展，就像人们能欣赏顶级运动员比赛，即便自身运动水平不高，Inverse RL中的Reward Model也能大规模收集数据。
2. **找到更具泛化能力的解决方案**：在DeepSeek R1数学任务中，基于规则（Data-Driven）的奖励模型给予LLM（大语言模型）充分自由度，探索成功的答题模式。模型能发现“长链式思维”可提高回答正确率，保持可泛化的解题能力。在此，(Outcome) RM（奖励模型）是因，找到可泛化模式是果，而如何高效探索或学习这些模式作为因果间的媒介，影响学习效率，但不影响“能否学习”。
3. **作为Inference Time Optimization的基础**：在常规RL任务中，无“推理时间”和“训练时间”之分，多数RL在测试任务上训练，训练完部署系统做推理，每次生成动作只需网络前向传播，谈不上推理时间优化（如Mujoco/Atari任务）。在围棋任务中，没有仅通过神经网络推理就能击败人类顶级选手的RL策略，需策略网络配合值网络做蒙特卡洛树搜索（MCTS）。此时，值网络充当“密集奖励函数”，在推理中过滤不良动作。同理，在困难的LLM任务中，Reward Model也可作为推理时间的过滤器，与现有的后训练方法结合，提升LLM生成质量。

1. **可靠评估的重要性**：准确评估是算法改进基础，Online RL工具依赖可靠Reward Model才能生效。因RL算法收敛难、超参敏感、LLM训练慢，不可靠的Reward Model会导致优化实验结论不可信，甚至出现少量数据强行拟合称缩放定律的情况。
2. **不存在通用策略**：RL领域无万能策略，需依据任务特性（数据、奖励、系统、算力等）优化算法。DPO和GRPO的成功，在于发现先前系统冗余问题，并结合任务需求与硬件进行针对性改进，而非作为LLM时代策略优化的通用方案。

    观察角度：推理任务能切实提升模型的 “聪明” 程度，增强其跟随用户指令、完成任务以及解决问题的能力，从数学训练的模型来看，整体能力也得到了提升。
    动机角度：若能让大语言模型（LLM）具备推理能力，使其在行为上呈现出思考时间越长、正确率越高的特点，那么该系统或许能够实现自我提升（自举），就像数学家通过不断推理可以发现新定理、提出新问题并在解决问题方面取得进展。但使用不具备推理能力的模型来追求 “自我提升” 效果并以此宣传是不太合适的。

## 第二层：【Game】Experience from Games and Rule-based Tasks

这部分内容围绕大语言模型（LLM）的发展路径与挑战，从奖励模型的优化、数据瓶颈的突破，延伸到游戏作为新训练场景的潜力，具体如下：

### 一、奖励模型的局限性与风险

在LLM Post-Train阶段，通过人类经验、反馈或人工题库构建奖励模型，可将缺失奖励函数的MDP\R问题转化为完整MDP问题。数据驱动的方式不仅成本低廉、易于规模化，还能在数学任务中大幅提升模型的优化泛化性与通用能力。

然而，基于有限样本拟合的奖励函数存在过拟合风险，且不同模型、数据规模和任务类型下，风险程度各异。Reward Model过拟合会引发Reward Hacking现象，例如在“helpful”任务中，模型因发现“输出越长得分越高”，便大量输出无意义内容，背离奖励设计初衷。

### 二、短期缓解与长期困境

短期内，可通过类似减少模型过拟合、提升泛化性的方法，在有限范围内缓解Reward Hacking问题。但从长远来看，这种发展模式与数据和算力可预测的扩张趋势不符。在众多改进方向中，算法的创新与优化往往难以预测。

### 三、游戏：突破数据瓶颈的新方向

除数学任务外，游戏为突破LLM数据瓶颈、增强模型能力提供了新可能。游戏作为定义良好的完整MDP，早在十几年前就被用于训练DeepRL算法。将DeepRL算法应用于LLM，可借助游戏环境让模型在反复尝试中提升理解、推理、规划和指令跟随能力。

不同类型游戏各有优劣：

- **文字/辩论类游戏**：输入输出空间适配语言模型，但胜负判断标准较难界定；
- **棋牌类游戏**：规则明确，但输入输出空间的表征适配存在挑战；
- **复杂3D游戏**：对LLM和VLM能力要求高，需解决课程设计和任务选择等问题。

### 四、亟待解决的关键问题

目前，以游戏驱动LLM发展仍面临诸多挑战：

1. **任务评估**：如何确定最适合评估LLM能力的游戏任务？
2. **作弊防范**：怎样避免text-based game中的作弊行为？
3. **表征优化**：如何找到LLM处理游戏输入输出的最佳表示方式？
4. **能力均衡**：如何选择能全面发展LLM能力、避免过拟合的游戏？
5. **能力迁移**：游戏中取得的进展能否带来模型全面能力提升？
6. **工具使用**：调用外部工具（如AlphaGo的value function、GTO软件）时，LLM如何进行推理学习？造轮子与用轮子哪个更重要？
7. **规律探索**：是否存在类似“游戏至上缩放定律”的理论？游戏提升LLM推理能力的上限在哪里？

一旦解决上述问题，通过大规模Self-Play进一步提升LLM推理能力，算力将成为主要限制因素。

## 第三层：【Virtual Experience】“Experience” in the Virtual World

这部分内容围绕LLM+RL发展第三层“Virtual Experience”（虚拟体验）中Agent的相关探索，从研究现状、机遇、挑战等方面展开：

1. **Agent研究现状与趋势**：Agent是面向产品、用户和落地的课题，工程优化、用户反馈、社群建设维护至关重要，基座模型能力提升可带来质变。MCP到来后，适配与权限问题不再关键，数据拥有者会因市场选择对Agent更开放，前提是Agent强大且受用户支持，Agent时代的内容和社交领域或迎来格局洗牌。
2. **Agent在RL视角下的机遇**
    - **交互与反馈**：Agent与虚拟世界交互完成任务，增加了和非语言系统交互可能，交互产生的反馈真实、一手且on-policy，即自身的“Experience”。
    - **任务与泛化**：用户可定义多样任务并反馈结果，Agent参与场景贴近日常需求，无需担忧能力泛化。用户众包反馈能助力Agent能力提升，类似培养专业劳动者。
    - **Multi-Goal问题优势**：Agent达成目标属RL的Multi-Goal问题，便于从失败经验学习。相比LLM做数学题失败经验利用受限，Agent失败经验可灵活调整目标复用，学习效率更高 。
3. **Agent面临的技术挑战**
    - **持续学习**：如何注入可规模化的持续学习能力，其范式为何尚不明确。
    - **学习规律**：RL存在plasticity vanishment问题，GPT系列模型监督学习的scaling law在RL中是否适用存疑。
    - **工程与算力**：大规模Agent Learning面临工程和算力双重挑战，用Prompt赋予Agent多样性作用有限，Fine-tuning和不同预训练模型难以支撑。
    - **个性化实现**：Agentic Personalization是趋势，但缺乏端侧友好的轻量化方案，对齐和监管要求中心化处理，当前技术实现大规模中心化需大量算力支持。

## 第四层：【Physical Experience】“Experience” in the Physical World

这部分内容围绕机器人和具身智能的发展，探讨了硬件、伦理等方面的问题，具体如下：

1. **领域热度与情感**：机器人和具身智能近年再度火热，早期RL方向的人对其感情深厚，能与物理世界交互的机器人是未来趋势，但面临硬件和伦理挑战。
2. **硬件层面**：2020年尝试制作简易四足机器，因贸易战及小米同类产品竞争失败。硬件遵循一分钱一分货原则，重资产轻技术，廉价硬件难以精准操控，限制机器人应用。
3. **伦理层面**：智能（辅助）驾驶场景中，消费者不愿为智能“具身”支付过多硬件成本，车作为载体动作有限但相对可控。同时，存在伦理问题，如对车边开边学的接受度、对车犯错的责任界定，以及人与机器互信的建立。当智能通过具身与物理世界交互，AI安全问题迫切，AGI时代需要哲学指引。
