motion capture

三维重建 瓶颈 和物体的交互，简化问题

1. dct prediction iaa dct
指标traj error global下降 contact下降 滑步

方案 - prediction 部分下降和可视化double check

物体和人的contact - 约束 - 物体和人上接触，比如 人坐在椅子上

rude postprocess
dart

-> image prior or force post postprocess

时间问题：

2s生成时间，out of regress 另外步数比较少

交互的解决方案到底有哪些呢？这个领域的优缺点有哪些？

数据采集流程

？全局导航信息 -1. driven command 2. navigation points
end to end 输入导航系统
初始化随机放置ego vehicle作为起点。全局规划后栅格化为navigation waypoints

自车跟导航走生成10个navpoints，自车朝向

10fps

自车odom更新，同步navipoints的state update （pop queue）

目的：采集用户个性化数据
开车数据导航是否waypoint那么dense和精细，是否不利于采集个性化数据。

保留用户个性化风格。（不是吃金币）

Q. 真人采集？本身是可视化的方式问题

数据存储：

达不到10fps：真人采集 - 渲染帧率 - 渲染sensor （6cam calibra instan 6depth 1lidar 5radar）

存储和世界渲染 - offline？世界渲染是否是可以离线复现（轨迹，地图，时间戳，驾驶行为）

人操控车 - 帧率，存储时间，渲染丝滑程度，数据记录，交互行为

预期数据规模（20个人 - 熟练工 - carla多地图 - 5~10分钟），中间地图到下一个地图有10分钟左右warm-up

后续算法prepare？

visionbase end to end？

另一个问题：筛选交通灯和路牌。navigation waypoint要做

问题1：ego vehicle and pedestrian单进程获取流程打通，单进程很慢？
问题2：筛选交通信号灯和路牌等

问题简化：工程下简化问题，pipeline流程打通

drivediffsion以及数据集

openscenes数据下载，挺好的。一个网络解决行泊一体的场景端到端智驾。

隐式模型 - 相关工作 dpo正负样本对 reward model

隐式输出数值， llm参与进来结合human feedback指导rl finetune，无法理解隐式模型本身，只能处理数值。

latent reward

得到reward之后进行分解 得到total reward。而不是分步骤reward。最终给出之后llm的作用是reward 拆分，在中间步骤来辅助。

对human的响应没有那么好。训练，数据，微调

响应动作的速度问题是一个很大的瓶颈！

初始化定量查看性能表现，通过pretrain model后效果会好很多

success的底层reward function 用gt reward。sparse reward没有gt reward那么精准

eureka + RLHF + CL

human反馈的成功命中率很低（失败：反馈了然后，llm回去修改，llm的出错，函数头里面没有存在的变量？注入了奇怪的东西，llm的coding能力有问题？幻觉？）

deepseek api问题：r1的一次性不能生成多组回答，不能并行？可能是设计问题。

eureka + gpt4o

--------
fouundationpose ++

快速移动问题
旋转的姿态问题

-> refine 上一针pose crop当前帧rgbd

工程优化 显示的2d tracker进行重新的划分。2d tracker with icp，多视角icp，运动太快的问题，如果2d tracker可以ok的话，第一针pose register来用是否可以

机械臂左手，右手的规格？好像没有左右手。ur catcheat

egoview去做这个事情看到的比较有限。

-----
consistency 自回归和diffusion
连续和离散的比较

离散文本，连续图片，高维信息

二位图片领域 连续图片 noise robust

离散token更好处理文本

ar mask-ar var far
var：coarse to fine，离散tokenizer 多尺度变换 recovery成最终的生成图片

频域角度FAR，fancy

diffusion+ar

iclr best paper：domain in 1 dimension

transformer --> diffusion

图像是离散更好还是连续更好？

showlab show o: discrete understanding ？

文本 - 离散（自回归） 或 连续（diffusion based）

句子分块，block中结合diffusion，两种结合的优势可以unified。

vae-diffusion框架：连续性仍然能够保持

连续 - 离散 ：优势：自回归可以用文本一样的去处理图片，利用embedding能力

缺陷：压缩过程的信息损失。

信息损失可以作为一种残差，如果得到残差之后可以用diffusion处理后在下游concat，找回一部分丢失的信息损耗。（亡羊补牢）

kaiming he：别用离散query？

[...]

原始图片 + 高频滤波 -> 恢复高频信息

action领域：latency

离散token转化连续action

时频转换和diffusion在后面的轨迹控制和运动规划是否还有更多的创意？如何和deep learning相结合？

例如，vla 统一模型， transfusion，更多motion（temporal，traj）

dense-policy：粗力度 到 细粒度（var）
